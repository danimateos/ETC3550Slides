---
title: "Time Series & Forecasting"
author: "Ch3. Time series decomposition"
date: \today
toc: true
titlepage: titlepage-ie.jpg
output:
  binb::monash:
    fig_width: 7
    fig_height: 3.5
    includes:
      in_header: header.tex
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = TRUE,
  dev.args = list(pointsize = 11)
)
options(
  digits = 3,
  width = 60,
  ggplot2.continuous.colour="viridis",
  ggplot2.continuous.fill = "viridis",
  ggplot2.discrete.colour = c("#D55E00", "#0072B2","#009E73", "#CC79A7", "#E69F00", "#56B4E9", "#F0E442"),
  ggplot2.discrete.fill = c("#D55E00", "#0072B2","#009E73", "#CC79A7", "#E69F00", "#56B4E9", "#F0E442")
)
library(fpp3)
library(purrr)
library(transformr) # Just to get it on renv
library(gganimate)
library(latex2exp)
```

# Transformations and adjustments

## Per capita adjustments
\fontsize{13}{14}\sf

```{r gdp-per-capita}
global_economy %>%
  filter(Country == "United Arab Emirates") %>%
  autoplot(GDP)
```

## Per capita adjustments
\fontsize{13}{14}\sf

```{r gdp-per-capita2}
global_economy %>%
  filter(Country == "United Arab Emirates") %>%
  autoplot(GDP / Population)
```

<!--
## Your turn

Consider the GDP information in `global_economy`. Plot the GDP per capita for each country over time. Which country has the highest GDP per capita? How has this changed over time? -->

## Inflation adjustments
\fontsize{10}{10}\sf

```{r retail_cpi, message=FALSE, warning=FALSE, fig.show='hide'}
print_retail <- aus_retail %>%
  filter(Industry == "Newspaper and book retailing") %>%
  group_by(Industry) %>%
  index_by(Year = year(Month)) %>%
  summarise(Turnover = sum(Turnover))

aus_economy <- global_economy %>%
  filter(Code == "AUS")

print_retail %>%
  left_join(aus_economy, by = "Year") %>%
  mutate(Adjusted_turnover = Turnover / CPI * 100) %>%
  pivot_longer(c(Turnover, Adjusted_turnover),
               values_to = "Turnover") %>%
  ggplot(aes(x = Year, y = Turnover)) +
  geom_line() +
  facet_grid(name ~ ., scales = "free_y") +
  labs(title = "Turnover: Australian print media industry",
       y = "$AU")
```


## Inflation adjustments
\fontsize{10}{10}\sf

```{r ref.label = 'retail_cpi', message=FALSE, warning=FALSE, echo=FALSE, fig.height=5}
```

## Your turn
\fontsize{11}{13}\sf

* `aus_retail` contains data in $million AUD. Apply to the series `"Cafes, restaurants and catering services"` an inflation adjustment to see how the real (as opposed to nominal) spending has increased. Plot the resulting series (one line for all of Australia)

* In those years, the Australian population has increased. Join the Australian population data from `global_economy` and calculate the real (as opposed to nominal) adjusted spending per capita in `"Cafes, restaurants and catering services"` and `"Takeaway food services"`. 

<!--

```{r your_turn, message=FALSE, warning=FALSE, fig.show='hide'}
aus_stats = global_economy %>%
  filter(Code == "AUS") %>% 
  select(c(CPI, Population))

aus_retail %>% 
  group_by(Industry) %>% 
  summarise(Turnover = sum(Turnover))  %>%
  mutate(Year = year(Month)) %>% 
  left_join(aus_stats) %>%
  mutate(Adjusted = 100 * Turnover / (Population * CPI)) %>%
  filter(Industry== "Cafes, restaurants and catering services" | Industry == "Takeaway food services" ) %>%
  autoplot()
```

-->


## Mathematical transformations
\fontsize{13}{15}\sf

If the data show different variation at different levels of the series, then a transformation can be useful.
\pause

Denote original observations as $y_1,\dots,y_n$ and transformed
observations as $w_1, \dots, w_n$.
\pause

\begin{block}{\footnotesize Mathematical transformations for stabilizing
variation}
\begin{tabular}{llc}
Square root & $w_t = \sqrt{y_t}$ & $\downarrow$ \\[0.2cm]
Cube root & $w_t = \sqrt[3]{y_t}$ & Increasing \\[0.2cm]
Logarithm & $w_t = \log(y_t)$  & strength
\end{tabular}
\end{block}
\pause

Logarithms, in particular, are useful because they are more interpretable: changes in a log value are **relative (percent) changes on the original scale**.

## Mathematical transformations
\fontsize{12}{14}\sf

```{r food, echo=TRUE}
food <- aus_retail %>%
  filter(Industry == "Food retailing") %>%
  summarise(Turnover = sum(Turnover))
```

```{r food-plot, echo = FALSE, fig.height=3.4}
food %>% autoplot(Turnover) +
  labs(y = "Turnover ($AUD)")
```

## Mathematical transformations
\fontsize{12}{14}\sf

```{r food-sqrt1, echo=TRUE, fig.height=4}
food %>% autoplot(sqrt(Turnover)) +
  labs(y = "Square root turnover")
```

## Mathematical transformations
\fontsize{12}{14}\sf

```{r food-cbrt, echo=TRUE, fig.height=4}
food %>% autoplot(Turnover^(1/3)) +
  labs(y = "Cube root turnover")
```

## Mathematical transformations
\fontsize{12}{14}\sf

```{r food-log, echo=TRUE, fig.height=4}
food %>% autoplot(log(Turnover)) +
  labs(y = "Log turnover")
```

## Mathematical transformations
\fontsize{12}{14}\sf

```{r food-inverse, echo=TRUE, fig.height=4}
food %>% autoplot(-1/Turnover) +
  labs(y = "Inverse turnover")
```

## Your turn

* Plot the logarithm in base 10 of US GDP from the `global_economy` dataset. 

* What's the difference between the latest value and the first value? What does it mean?

<!-- 
```{r}
global_economy %>%
  filter(Code=="USA") %>% 
  autoplot(log10(GDP))
```
-->

## Box-Cox transformations

Each of these transformations is close to a member of the
family of \textbf{Box-Cox transformations}:
$$w_t = \left\{\begin{array}{ll}
        \log(y_t),      & \quad \lambda = 0; \\
        (y_t^\lambda-1)/\lambda ,         & \quad \lambda \ne 0.
\end{array}\right.
$$\pause

* $\lambda=1$: (No substantive transformation)
* $\lambda=\frac12$: (Square root plus linear transformation)
* $\lambda=0$: (Natural logarithm)
* $\lambda=-1$: (Inverse plus 1)

## Box-Cox transformations

```{r food-anim, cache=TRUE, echo=FALSE, fig.show='animate', interval=1/10, message=FALSE, fig.height=5, fig.width=8, aniopts='controls,buttonsize=0.3cm,width=11.5cm'}
food %>%
  mutate(!!!set_names(map(seq(0, 1, 0.01), ~ expr(fabletools::box_cox(Turnover, !!.x))), seq(0, 1, 0.01))) %>%
  select(-Turnover) %>%
  pivot_longer(-Month, names_to = "lambda", values_to = "Turnover") %>%
  mutate(lambda = as.numeric(lambda)) %>%
  ggplot(aes(x = Month, y = Turnover)) +
  geom_line() +
  transition_states(1 - lambda, state_length = 0) +
  view_follow() +
  labs(title = "Box-Cox transformed food retailing turnover (lambda = {format(1 - as.numeric(closest_state), digits = 2)})")
```

## Box-Cox transformations
\fontsize{12}{14}\sf


```{r food-lambda, echo=TRUE}
food %>%
  features(Turnover, features = guerrero)
```

\pause\fontsize{13}{15}\sf

* This attempts to balance the seasonal fluctuations and random variation across the series.
* Always check the results.
* A low value of $\lambda$ can give extremely large prediction intervals.

## Box-Cox transformations
\fontsize{13}{14}\sf

```{r food-bc, echo=TRUE,fig.height=4}
food %>% autoplot(box_cox(Turnover, 0.0524)) +
  labs(y = "Box-Cox transformed turnover")
```

## Transformations
\fontsize{13}{15}\sf

* Often no transformation needed.
* Simple transformations are easier to explain and work well enough.
* Transformations can have very large effect on PI.
* If some data are zero or negative, then use $\lambda>0$.
* `log1p()` can also be useful for data with zeros.
* Choosing logs is a simple way to force forecasts to be positive
* Transformations must be reversed to obtain forecasts on the original scale. (Handled automatically by `fable`.)

## Your turn
\fontsize{11}{13}\sf

* Play around with the lambda slider at [https://otexts.com/fpp3/transformations.html](https://otexts.com/fpp3/transformations.html) (near the bottom; you can search for "slider"). 
  * How does the behavior of the transformation change between between very negative values of lambda and and very high positive values?
  * What value represents the cutoff between these two behaviors?


## Your turn
\fontsize{11}{13}\sf

* Create the variable `a10_spending` that reflects the total spending in diabetic drugs (ATC2 "A10") from the `PBS` dataset. 
  * Plot it. Do you think it needs a transformation? Which one of the ones we saw earlier (square, inverse, log) looks better intuitively?
  * Use the `features` function in combination with `guerrero` to calculate the optimal lambda. Does it match your intuition?

<!-- ## Your turn

\fontsize{13}{14}\sf

1. For the following series, find an appropriate transformation in order to stabilise the variance.

    * United States GDP from `global_economy`
    * Slaughter of Victorian “Bulls, bullocks and steers” in `aus_livestock`
    * Victorian Electricity Demand from `vic_elec`.
    * Gas production from `aus_production`

2. Why is a Box-Cox transformation unhelpful for the `canadian_gas` data?
 -->

# Moving averages

## Moving average of order m

$$\hat T_t = \frac{1}{m} \sum_{j=-k}^{k}y_{t+j}$$ 


## Moving averages in R
\fontsize{9}{10}\sf

The longer the period we average, the smoother the line.

```{r}
smoothed = us_employment %>% 
  filter(Title=="Total Private", year(Month) > 2000) %>% 
  mutate(`5-MA` = slider::slide_dbl(Employed, mean, .before=2, .after=2),
         `12-MA` = slider::slide_dbl(Employed, mean, .before=6, .after=5),
         `15-MA` = slider::slide_dbl(Employed, mean, .before=7, .after=7)) %>%
  mutate(`5x5-MA` = slider::slide_dbl(`5-MA`, mean, .before=2, .after=2),
         `12x12-MA` = slider::slide_dbl(`15-MA`, mean, .before=5, .after=6))
```

## Moving averages in R
\fontsize{9}{10}\sf

```{r}
smoothed %>% 
  autoplot(Employed) + 
    geom_line(aes(y=`5-MA`), color='red') + 
    geom_line(aes(y=`12-MA`), color='blue') +
    geom_line(aes(y=`15-MA`), color='green')
```


## Moving averages of moving averages
\fontsize{9}{10}\sf

Compare the 12x12-MA with the 12-MA:

```{r}
smoothed %>% 
  autoplot(Employed) + 
    geom_line(aes(y=`15-MA`), color='red') + 
    geom_line(aes(y=`12x12-MA`), color='blue') + 
    geom_line(aes(y=`12-MA`), color='green') 

```

## Moving averages with stock market data
\fontsize{9}{10}\sf

```{r}
goog_ma = gafa_stock %>% 
  filter(Symbol=="GOOG") %>%
  mutate(`10-MA` = slider::slide_dbl(Close, .before=10, mean),
         `50-MA` = slider::slide_dbl(Close, .before=50, mean),
         `200-MA` = slider::slide_dbl(Close, .before=200, mean)) 

```

## Moving averages with stock market data
\fontsize{9}{10}\sf

```{r}
goog_ma %>%
  autoplot(Close) + 
    geom_line(aes(y=`10-MA`), color="red", linetype="dashed") +
    geom_line(aes(y=`50-MA`), color="blue", linetype="dashed") +
    geom_line(aes(y=`200-MA`), color="green")
```


# Time series components

## Time series patterns

**Recall**

Trend
:  pattern exists when there is a long-term increase or decrease in the data.

Cyclic
: pattern exists when data exhibit rises and falls that are *not of fixed period* (duration usually of at least 2 years).

Seasonal
: pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week).

## Time series decomposition

\begin{block}{}\vspace*{-0.3cm}
\[ y_t = f(S_t, T_t, R_t) \]
\end{block}
\begin{tabular}{@{}llp{8cm}@{}}
where & $y_t=$ & data at period $t$ \\
      & $T_t=$ & trend-cycle component at period $t$\\
      & $S_t=$ & seasonal component at period $t$ \\
      & $R_t=$ & remainder component at period $t$
\end{tabular}
\pause

**Additive decomposition:** $y_t = S_t + T_t + R_t.$

**Multiplicative decomposition:** $y_t = S_t \times T_t \times R_t.$

## Time series decomposition
\fontsize{13}{15}\sf

  *  Additive model  appropriate if  magnitude of  seasonal fluctuations does not vary with level.
  *  If seasonal are proportional to level of series, then multiplicative model appropriate.
  *  Multiplicative decomposition more prevalent with economic series
  *  Alternative: use a Box-Cox transformation, and then use additive decomposition.
  *  Logs turn multiplicative relationship into an additive relationship:

$$y_t = S_t \times T_t \times R_t \quad\Rightarrow\quad
\log y_t = \log S_t + \log T_t + \log R_t.
$$

## US Retail Employment
\fontsize{10}{11}\sf

```{r usretail}
us_retail_employment <- us_employment %>%
  filter(year(Month) >= 1990, Title == "Retail Trade") %>%
  select(-Series_ID)
us_retail_employment
```

\vspace*{10cm}

## US Retail Employment
\fontsize{10}{11}\sf

```{r dable1}
us_retail_employment %>%
  autoplot(Employed) +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail")
```

\vspace*{10cm}

## US Retail Employment
\fontsize{10}{11}\sf

```{r dable2}
us_retail_employment %>%
  model(stl = STL(Employed))
```

\vspace*{10cm}

## US Retail Employment
\fontsize{10}{11}\sf

```{r dable3}
dcmp <- us_retail_employment %>%
  model(stl = STL(Employed))
components(dcmp)
```

\vspace*{10cm}

## US Retail Employment
\fontsize{10}{11}\sf

```{r dable4}
us_retail_employment %>%
  autoplot(Employed, color='gray') +
  autolayer(components(dcmp), trend, color='#D55E00') +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail")
```

\vspace*{10cm}

## US Retail Employment
\fontsize{10}{11}\sf

```{r usretail-stl, fig.width=8, fig.height=5}
components(dcmp) %>% autoplot()
```

## US Retail Employment
\fontsize{10}{11}\sf

```{r usretail3}
components(dcmp) %>% gg_subseries(season_year)
```

## Seasonal adjustment

  *  Useful by-product of decomposition:  an easy way to calculate seasonally adjusted data.
  *  Additive decomposition: seasonally adjusted data given by
$$y_t - S_t = T_t + R_t$$
  *  Multiplicative decomposition: seasonally adjusted data given by
$$y_t / S_t = T_t \times R_t$$

## US Retail Employment
\fontsize{10}{11}\sf

```{r usretail-sa}
us_retail_employment %>%
  autoplot(Employed, color='gray') +
  autolayer(components(dcmp), season_adjust, color='#0072B2') +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail")
```

## Seasonal adjustment

  * We use estimates of $S$ based on past values to seasonally adjust a current value.
  *  Seasonally adjusted series reflect **remainders** as well as **trend**. Therefore they are not "smooth" and "downturns" or "upturns" can be misleading.
  *  It is better to use the trend-cycle component to look for turning points.

# History of time series decomposition

## History of time series decomposition

\fontsize{13}{15}\sf

  *  Classical method originated in 1920s.
  *  Census II method introduced in 1957. Basis for X-11 method and variants (including X-12-ARIMA, X-13-ARIMA)
  *  STL method introduced in 1983
  *  TRAMO/SEATS introduced in 1990s.
\pause

### National Statistics Offices
 * ABS uses X-12-ARIMA
 * US Census Bureau uses X-13ARIMA-SEATS
 * Statistics Canada uses X-12-ARIMA
 * ONS (UK) uses X-12-ARIMA
 * EuroStat use X-13ARIMA-SEATS

## X-11 decomposition

**Advantages**

  *  Relatively robust to outliers
  *  Completely automated choices for trend and seasonal changes
  *  Very widely tested on economic data over a long period of time.

\pause

**Disadvantages**

  *  No prediction/confidence intervals
  *  Ad hoc method with no underlying model
  *  Only developed for quarterly and monthly data

## Extensions: X-12-ARIMA and X-13-ARIMA

  *  The X-11, X-12-ARIMA and X-13-ARIMA methods are based on Census II decomposition.
  *  These allow adjustments for trading days and other explanatory variables.
  *  Known outliers can be omitted.
  *  Level shifts and ramp effects can be modelled.
  *  Missing values estimated and replaced.
  *  Holiday factors (e.g., Easter, Labour Day) can be estimated.

## X-13ARIMA-SEATS

**Advantages**

  * Model-based
  * Smooth trend estimate
  * Allows estimates at end points
  * Allows changing seasonality
  * Developed for economic data

\pause

**Disadvantages**

  *  Only developed for quarterly and monthly data

# STL decomposition

## STL decomposition

\fontsize{13}{14}\sf

  *  STL: "Seasonal and Trend decomposition using Loess"
  *  Very versatile and robust.
  *  Unlike X-12-ARIMA, STL will handle any type of seasonality.
  *  Seasonal component allowed to change over time, and rate of change controlled by user.
  *  Smoothness of trend-cycle also controlled by user.
  *  Robust to outliers
  *  Not trading day or calendar adjustments.
  *  Only additive.
  *  Take logs to get multiplicative decomposition.
  *  Use Box-Cox transformations to get other decompositions.

## STL decomposition
\fontsize{10}{11}\sf

```{r stlwindow9, echo=TRUE, warning=FALSE, fig.width=8, fig.height=4}
us_retail_employment %>%
  model(STL(Employed ~ season(window=9), robust=TRUE)) %>%
  components() %>% autoplot() +
    labs(title = "STL decomposition: US retail employment")
```

## STL decomposition
\fontsize{10}{11}\sf

```{r stlwindowanim, echo=FALSE, warning=FALSE, message=FALSE, fig.show='animate', interval=1/10,  fig.height=5.35, fig.width=8, aniopts='controls,buttonsize=0.3cm,width=11.5cm', eval=TRUE}
s_windows <- seq(5,55,by=2)
stl_defs <- purrr::map(s_windows, function(s_window){
  STL(Employed ~ season(window=s_window), robust=TRUE)
})
names(stl_defs) <- sprintf("season(window=%02d)", s_windows)

us_retail_employment %>%
  model(!!!stl_defs) %>%
  components() %>%
  as_tibble() %>%
  pivot_longer(Employed:remainder,
               names_to = "component", names_ptypes = list(component = factor(levels = c("Employed", "trend", "season_year", "remainder"))),
               values_to = "Employed") %>%
  ggplot(aes(x = Month, y = Employed)) +
  geom_line() +
  facet_grid(rows = vars(component), scales = "free_y") +
  labs(title = "STL decomposition of US retail employment",
       subtitle = "{closest_state}") +
  transition_states(.model)
```

\vspace*{10cm}

## STL decomposition
\fontsize{10}{11}\sf

```{r echo = TRUE, results = 'hide'}
us_retail_employment %>%
  model(STL(Employed ~ season(window=5))) %>%
  components()

us_retail_employment %>%
  model(STL(Employed ~ trend(window=15) +
                       season(window="periodic"),
            robust = TRUE)
  ) %>% components()
```

\fontsize{12}{13}\sf

  *  `trend(window = ?)` controls wiggliness of trend component.
  *  `season(window = ?)` controls variation on seasonal component.
  *  `season(window = 'periodic')` is equivalent to an infinite window.

## STL decomposition
\fontsize{10}{11}\sf

```{r mstl, fig.width=8, fig.height=4}
us_retail_employment %>%
  model(STL(Employed)) %>%
  components() %>%
  autoplot()
```

* `STL()` chooses `season(window=13)` by default
* Can include transformations.

## STL decomposition
\fontsize{13}{14.5}\sf

* Algorithm that updates trend and seasonal components iteratively.
* Starts with $\hat{T}_t=0$
* Uses a mixture of loess and moving averages to successively refine the trend and seasonal estimates.
* The trend window controls loess bandwidth applied to deasonalised values.
* The season window controls loess bandwidth applied to detrended subseries.
* Robustness weights based on remainder.
* Default season `window = 13`
* Default trend `window = nextodd(` \newline\mbox{}\hfill `ceiling((1.5*period)/(1-(1.5/s.window)))`


## Your turn
\fontsize{11}{12}\sf

* Let's play around with the `vic_elec` dataset. Group it by day and average `Demand` so that we can ignore the intraday seasonality. Keep only the year 2014.

* Decompose it using STL and plot the `components`. What do you see?

* Play with the window values for both season and trend. 
  * Which value makes the trend more/less wiggly? 
  * Which one makes the seasonal component change faster/slower? 
  * Wich combination minimizes the values left in the remainder without producing absurdly looking trend/seasons?

<!--
```{r}
daily = vic_elec %>% 
  as_tibble() %>% 
  group_by(Date) %>% 
  summarise(avg_demand = mean(Demand)) %>%
  as_tsibble(index=Date) %>% 
  filter(year(Date) == 2014)

daily %>% 
  filter(year(Date)==2014) %>% 
  model(STL(avg_demand ~ trend(window=21) + season(window=11))) %>% 
  components() %>% 
  autoplot
```
-->

<!--
https://stats.stackexchange.com/questions/253917/why-use-differencing-and-box-cox-in-time-series
https://en.wikipedia.org/wiki/Variance-stabilizing_transformation
-->
